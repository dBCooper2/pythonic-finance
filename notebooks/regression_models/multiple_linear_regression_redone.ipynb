{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Multiple Linear Regression in Python__\n",
    "\n",
    "By: Trevor Rowland ([dBCooper2](https://github.com/dBCooper2))\n",
    "\n",
    "Creating a Multiple Linear Regression Model from Scratch\n",
    "\n",
    "Expanding on the Simple Linear Regression notebook, this notebook aims to implement a Multivariate Linear Regression Model for use in Fama-French 3-Factor and 5-Factor Analysis\n",
    "\n",
    "Notation Definitions:\n",
    "\n",
    "- Capital Letters are Vectors/Matrices\n",
    "- Lowercase Letters are Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models take a series of predictor(X) variables and a single response(Y) variable, and estimates a line of best fit that can be used to predict unknown response variables.\n",
    "\n",
    "This regression model can be applied to any series of predictor and response variables, however for the purpose of the [pythonic-finance project]() This model will be used in the Fama-French 3 and 5 factor analyses of portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a. _The 3-Factor Model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fama-French 3 Factor Model is an extension of the Capital Asset Pricing Model, aiming to describe a stock or portfolio's returns through market risk as well as the outperformance of small-cap companies relative to large-cap companies and the outperformance of high market-to-book value companies relative to low market-to-book value companies.\n",
    "\n",
    "The model suggests that both small-cap stocks and stocks with a high market-to-book ratio tend to regularly outperform the overall market, and thus should be factored into the model.\n",
    "\n",
    "This data can be found in [Dr. Kenneth French's]() data library and will be used for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the 3-factor model is:\n",
    "\n",
    "$$ R_i-R_{rf} = \\alpha + \\beta_1(R_{rf}-R_m) + \\beta_2(SMB) + \\beta_3(HML) + \\epsilon_i$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$R_i$ is the expected rate of return\n",
    "\n",
    "$R_{rf}$ is the risk-free rate\n",
    "\n",
    "$SMB$ = _Small Minus Big_, the historic excess returns of small-caps over large-caps\n",
    "\n",
    "$HML$ = _High Minus Low_, the historic excess returns of high market-to-book ratio companies over low market-to-book ratio companies\n",
    "\n",
    "$\\beta_{1,2,3}$ are the coefficients of each factor, estimated by the regression model\n",
    "\n",
    "$\\alpha$ is the excess return on investment\n",
    "\n",
    "$\\epsilon_i$ is the noise within the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b. _The 5-Factor Model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fama-French 5-Factor model is another iteration of the 3-Factor Model, including 2 new factors. These are:\n",
    "\n",
    "- $RMW$ = _Robust Minus Weak_, the average return on two robust operating-profitability portfolios minus the average return on two weak operating-profitability portfolios.\n",
    "\n",
    "- $CMA$ = _Conservative Minus Aggressive_, the average return on two conservative investment portfolios minus the average return on two aggressive investment portfolios.\n",
    "\n",
    "These Factors are also found in [Dr. Kenneth French's]() data library, and will be used for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the 5-Factor Model is:\n",
    "\n",
    "$$ R_i-R_{rf} = \\alpha + \\beta_1(R_{rf}-R_m) + \\beta_2(SMB) + \\beta_3(HML) + \\beta_4(RMW) + \\beta_5(CMA) + \\epsilon_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simple linear regression model, it was easy to calculate the gradient descent as there were only 2 partial derivatives to calculate. For the multiple regression model, there are 4 and 6 partial derivatives for the Fama-French Models. The derivatives are with respect to the 3-5 predictor variables, and with respect to the alpha, or y-intercept of the regression line.\n",
    "\n",
    "Additionally, for the Fama-French Regression Class and future Regression Models, it is necessary to have an abstract Regression Model that can handle an indeterminate number of predictor variables. This means the derivation of the error function must be done in a way that can be translated to an array of size $n$ in Python. This requires the use of Matrices to simplify the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a. _The Model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multiple Linear Regression Model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+...+\\beta_px_p + \\epsilon_i$$\n",
    "\n",
    "Which can be translated into the Matrix Form:\n",
    "\n",
    "$$\n",
    "Y_i = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 & \\beta_1 & ... & \\beta_p\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "X_0 \\\\\n",
    "X_1 \\\\\n",
    "... \\\\\n",
    "X_p \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    ", X_0 = 1\n",
    "$$\n",
    "\n",
    "Setting $X_0 = 1$ allows the matrices to be the same size, which simplifies the calculations by including the Y-intercept Beta($\\beta_0$) in the coefficient matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b. _The Error Function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error function that will be minimized in the model is the Sum of Squared Errors, which measures variation within a cluster of data.\n",
    "\n",
    "The Formula for the Sum Squared Errors(SSE) is: $$E = SSE = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n}(y_i-\\hat{y_i})^2$$\n",
    "\n",
    "This is a sum of each of the squared differences between the observed response variable $y_i$ and the estimated response variable $\\hat{y_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Matrix form of the SSE formula is:\n",
    "\n",
    "$$ E = SSE = \\sum_{i=1}^{n} \\epsilon_i^2 = \\mathcal{E}^T\\mathcal{E}$$\n",
    "\n",
    "Instead of squaring the matrices, the error matrix is multiplied by its transpose. This is done because the errors are an $n{\\times}1$ matrix, and computing $\\epsilon_i^2$ is not possible, so the transpose is used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An expansion of this equation using vectors is provided below:\n",
    "\n",
    "$$E =\n",
    "\\sum_{i=1}^{n}\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\hat{y_1}& \n",
    "y_2 - \\hat{y_2}&  \n",
    "... &  \n",
    "y_n - \\hat{y_n}\n",
    "\\end{bmatrix} \n",
    "\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\hat{y_1}\\\\\n",
    "y_2 - \\hat{y_2}\\\\\n",
    "\\vdots \\\\\n",
    "y_n - \\hat{y_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c. _Computing the Error Function for the MLR Model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Algebra, the transpose of a sum can be decomposed in the following ways:\n",
    "\n",
    "$$(A+B)^T = A^T+B^T$$\n",
    "\n",
    "$$(A-B)^T = A^T-B^T$$\n",
    "\n",
    "Which means the transpose operator in $E = \\hat{\\mathcal{E}}^T\\hat{\\mathcal{E}}$ can be distributed, making the function:\n",
    "\n",
    "$$ E = \\sum_{i=1}^{n}(Y^T-\\hat{Y}^T)(Y-\\hat{Y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting the matrix form $\\hat{Y} = X \\beta$ into the error function returns:\n",
    "\n",
    "$$ E = \\sum_{i=1}^{n}(Y^T-(X \\beta)^T)(Y-(X \\beta))$$\n",
    "\n",
    "$$ E = \\sum_{i=1}^{n}[Y^T Y - Y^T X \\beta - Y(X \\beta)^T + (X \\beta)^T (X \\beta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to finish simplifying the equations, the following terms must be proven equal in order to simplify into the solution $\\hat{\\beta} = (X^T X^{-1})(X^T Y)$:\n",
    "\n",
    "$$(X \\beta)^T Y = Y^T (X \\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Y = A, X \\beta=B$:\n",
    "\n",
    "Therefore the equation $(X \\beta)^T Y = Y^T (X \\beta)$ becomes $A^T B = B^T A$\n",
    "\n",
    "By Linear Algebra, \n",
    "\n",
    "$$ (AB)^T = B^T A^T, (A+B)^T = A^T + B^T $$\n",
    "$$ (A^T B)^T = B^T A, (A-B)^T = A^T - B^T $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ A^T B = B^T A = (A^T B)^T $$\n",
    "\n",
    "$$Y^T (X \\beta)  = (Y^T (X \\beta))^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting this back into the SSE Equation allows it to be simplified:\n",
    "\n",
    "$$ E = \\sum_{i=1}^{n} Y^T Y - Y^T X \\beta - Y(X \\beta)^T + (X \\beta)^T (X \\beta)$$\n",
    "\n",
    "$$ E = \\sum_{i=1}^{n} Y^T Y - 2Y^T X \\beta + (X \\beta)^T (X \\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.d. _The Partial Derivative of the Error Function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the error function is expanded to include the equation of the MLR Model, the partial derivative of the error function can be computed.\n",
    "\n",
    "The partial derivative is used to compute how much the error within the model is changing, and is iteratively calculated to minimize each coefficient $\\beta$. It is important to note that this partial derivative is merely an estimate, as the data is a series of discrete observations and not continuous.\n",
    "\n",
    "The vector of the minimized values of each $\\beta$ are labeled $\\hat{\\beta}$ in the _Normal Equations_, which are the results of the minimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial E}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} ( \\sum_{i=1}^{n} Y^T Y - 2Y^T X \\beta + (X \\beta)^T (X \\beta)) $$\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial \\beta} = \\sum_{i=1}^{n} 0 - 2Y^T X + 2X^T \\beta^T X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then setting the partial derivative equal to 0 and solving for $\\hat{\\beta}$, the equation becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 0 = 0 - 2Y^T X + X^T \\hat{\\beta}^T X$$\n",
    "\n",
    "$$ 2Y^T X = X^T \\hat{\\beta}^T X$$\n",
    "\n",
    "$$ \\hat{\\beta}^T = \\frac{2Y^T X}{2X^T X}  = (Y^T X)(X^T X)^{-1}$$\n",
    "\n",
    "$$ \\hat{\\beta} = [(Y^T X)(X^T X)^{-1}]^T $$\n",
    "\n",
    "$$ \\hat{\\beta} = (Y^T X)^T [(X^T X)^{-1}]^T $$\n",
    "\n",
    "$$ \\hat{\\beta} = (X^T Y) (X^T X^{-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the Normal Equations can be found by rearranging the equation:\n",
    "$$X^TX\\hat{\\beta} = X^T Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.e. _Interpreting the Theory to Translate into an Algorithm_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\beta}$ is the _Least Squares Estimator_ for the model. This means that it is a coefficient matrix that can take the observed $x$ and $y$ values and estimate a value for each $\\beta$ in the model.\n",
    "\n",
    "This is a great way to show how the regression line is being calculated, but for the Python program, the estimations occurring need to be examined at a more granular level. This birds-eye view shows what is happening to the matrices, but for the program, the individual indices must be examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Gradient Descent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Simple Linear Regression Model, the Formula for the Gradient Descent of the slope was:\n",
    "\n",
    "$$m_{new} = m_{current} - \\frac{\\partial E}{\\partial m}$$\n",
    "\n",
    "Because the Multiple Linear Regression Model is composed of multiple slopes, a more vector form of this equation must be constructed that uses $\\beta$ for each of the slopes. This formula can be represented as:\n",
    "\n",
    "$$\\beta_{new} = \\beta_{current} - \\frac{\\partial E}{\\partial \\beta}$$\n",
    "\n",
    "Where i is the number of estimators in the regression equation.\n",
    "\n",
    "Lastly, a learning rate, $L$, needs to be added to the equation. This will be used in the iterative steps of the Python program that will be written. This means that the completed Gradient Descent Function is:\n",
    "\n",
    "$$\\beta_{new} = \\beta_{current} - L \\frac{\\partial E}{\\partial \\beta} $$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
