{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Multiple Linear Regression in Python__\n",
    "\n",
    "By: Trevor Rowland ([dBCooper2](https://github.com/dBCooper2))\n",
    "\n",
    "Creating a Multiple Linear Regression Model from Scratch\n",
    "\n",
    "Expanding on the Simple Linear Regression notebook, this notebook aims to implement a Multivariate Linear Regression Model for use in Fama-French 3-Factor and 5-Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _References:_\n",
    "\n",
    "[Gradient Descent Tutorial](https://www.machinelearningworks.com/tutorials/gradient-descent)\n",
    "\n",
    "[Multiple Linear Regression from Scratch - Machine Learning Math & Python](https://youtu.be/fldD6fGmsQE?si=IwQntHRUuJCFB-iz) by [kai](https://www.youtube.com/@dylankailau6672)\n",
    "\n",
    "[Statistics 101: Multiple Linear Regression, The Very Basics ðŸ“ˆ](https://youtu.be/dQNpSa-bq4M?si=9vpoTxdyGzZEPGOx) by Brandon Foltz\n",
    "\n",
    "[Statistics 101: Multiple Linear Regression, Data Preparation](https://youtu.be/2I_AYIECCOQ?si=axl8PUqk-JUR8QQn) by Brandon Foltz\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Introduction: From the Multiple Linear Regression Series by Brandon Foltz_\n",
    "\n",
    "##### __TODO__: Remove This/Combine with Derivation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models take a series of predictor(X) variables and a single response(Y) variable, and estimates a line of best fit that can be used to predict unknown response variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the Multiple Regression Model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+...+\\beta_px_p + \\epsilon_i$$\n",
    "\n",
    "For the multiple regression equation, $\\epsilon_i$ is assumed to be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Data Preparation_\n",
    "\n",
    "##### __TODO__: Move below the derivation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more predictor variables to a regression model often leads to _overfitting_ the data to the model. This is because variables may not be related to the response variables, or the predictors could me multi-collinear, meaning that the predictors are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Fama-French Models, this work has already been done, but for building custom models, the relationships between variables need to be carefully examined _before_ performing the multiple regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for a multiple linear regression model, the following steps must be performed:\n",
    "\n",
    "1. Generate a list of predictor(independent) and response(dependent) variables\n",
    "\n",
    "2. Collect data on the variables\n",
    "\n",
    "3. Examing the relationships between each predictor variable and the response variable using scatterplots and correlations\n",
    "\n",
    "4. Examine the relationships between each predictor variable using scatterplots and correlations to check for multicollinearity\n",
    "\n",
    "5. Optionally, perform simple linear regression for each predictor/response pair\n",
    "\n",
    "6. Using the non-redundant predictor variables, perform multiple linear regression and find the best-fitting model\n",
    "\n",
    "7. Use the best-fitting model to make predictions about the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Deriving the Gradient Descent Formula_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simple linear regression model, it was easy to calculate the gradient descent as there were only 2 partial derivatives to calculate. For the multiple regression model, there are 4 and 6 partial derivatives for the Fama-French Models. The derivatives are with respect to the 3-5 predictor variables, and with respect to the alpha, or y-intercept of the regression line.\n",
    "\n",
    "Additionally, for the Fama-French Regression Class and future Regression Models, it is necessary to have an abstract Regression Model that can handle an indeterminate number of predictor variables. This means the derivation of the error function must be done in a way that can be translated to an array of size $n$ in python. This requires the use of Matrices to simplify the calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multiple Linear Regression Model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+...+\\beta_px_p + \\epsilon_i$$\n",
    "\n",
    "Which can be translated into the Matrix Form:\n",
    "\n",
    "$$\n",
    "Y_i = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 & \\beta_1 & ... & \\beta_p\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "X_0 \\\\\n",
    "X_1 \\\\\n",
    "... \\\\\n",
    "X_p \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    ", X_0 = 1\n",
    "$$\n",
    "\n",
    "Setting $X_0 = 1$ allows the matrices to be the same size, which simplifies the calculations by including the Y-intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Deriving the Multiple Linear Regression Model Using Vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive the Multiple Linear Regression Model's Error Function, the Partial Derivatives of the Error Function must be computed. This was done using the [Matrix Form Multiple Linear Regression MLR](https://youtu.be/Imjfp1cxy6g?si=gWXnA9F_XisVzFA4) Tutorial by [Boer Commander](https://www.youtube.com/@BoerCommander), and the intermediate steps were done on paper to understand the process of developing these equations better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Model and the SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multiple Linear Regression Model is: $$y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+...+\\beta_px_p + \\epsilon_i$$\n",
    "\n",
    "The Formula for the Sum Squared Errors(SSE) is: $$SSE = \\sum_{n}^{i=1}(Y_i-\\hat{Y_i})$$\n",
    "\n",
    "The Vector Representation of the MLR Model is:\n",
    "\n",
    "$$Y = X\\hat{\\beta}+\\mathcal{E}$$\n",
    "\n",
    "Which can be expanded to show the individual observations within each random variable:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\ y_2\\\\ ...\\\\ y_n\n",
    "\\end{bmatrix}\n",
    "\n",
    " = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\ \\beta_1\\\\ ...\\\\ \\beta_p\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & ... & x_{1p} \\\\\n",
    "1 & x_{21} & x_{22} & ... & x_{2p} \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "1 & x_{n1} & x_{n2} & ... & x_{np} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\ \\epsilon_2\\\\ ...\\\\ \\epsilon_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "While the Errors($\\epsilon$) are important to take note of, they will not be included in the calculations of each partial derivative\n",
    "\n",
    "The Vector Representation of the SSE Formula is:\n",
    "\n",
    "$$SSE = \\hat{\\mathcal{E}}^T\\hat{\\mathcal{E}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting the Vector Representation of the MLR Model into the SSE returns:\n",
    "\n",
    "$$SSE = (Y - X\\hat{\\beta})^T (Y - X\\hat{\\beta})$$\n",
    "\n",
    "Simplifying $Y - X\\hat{\\beta}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\ y_2\\\\ \\vdots\\\\ y_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "-\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_p\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & ... & x_{1p} \\\\\n",
    "1 & x_{21} & x_{22} & ... & x_{2p} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & ... & x_{np} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "=\n",
    "\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\ y_2\\\\ \\vdots\\\\ y_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "-\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_p\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 & \\beta_1x_{11} & \\beta_2x_{12} & ... & \\beta_p x_{1p} \\\\\n",
    "\\beta_0 & \\beta_1x_{21} & \\beta_2x_{22} & ... & \\beta_p x_{2p} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "\\beta_0 & \\beta_1x_{n1} & \\beta_2x_{n2} & ... & \\beta_p x_{np} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "=\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\beta_0 - \\beta_1x_{11} - \\beta_2x_{12} - ... - \\beta_p x_{1p}\\\\ \n",
    "y_2 - \\beta_0 - \\beta_1x_{21} - \\beta_2x_{22} - ... - \\beta_p x_{2p}\\\\ \n",
    "\\vdots \\\\ \n",
    "y_n - \\beta_0 - \\beta_1x_{n1} - \\beta_2x_{n2} - ... - \\beta_p x_{np}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Makes $(Y - X\\hat{\\beta})^T$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\beta_0 - \\beta_1x_{11} - \\beta_2x_{12} - ... - \\beta_p x_{1p} & \n",
    "y_2 - \\beta_0 - \\beta_1x_{21} - \\beta_2x_{22} - ... - \\beta_p x_{2p} &  \n",
    "... &  \n",
    "y_n - \\beta_0 - \\beta_1x_{n1} - \\beta_2x_{n2} - ... - \\beta_p x_{np}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this, $(Y - X\\hat{\\beta})^T(Y - X\\hat{\\beta})$ becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "(y_1 - \\beta_0 - \\beta_1x_{11} - \\beta_2x_{12} - ... - \\beta_p x_{1p})^2\\\\ \n",
    "(y_2 - \\beta_0 - \\beta_1x_{21} - \\beta_2x_{22} - ... - \\beta_p x_{2p})^2\\\\ \n",
    "\\vdots \\\\ \n",
    "(y_n - \\beta_0 - \\beta_1x_{n1} - \\beta_2x_{n2} - ... - \\beta_p x_{np})^2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the partial derivative of each vector index, the partial derivative of one expression will be sufficient.\n",
    "\n",
    "The expressions can be represented as:\n",
    "\n",
    "$$(y_i - \\beta_0 - \\beta_1x_{i,1} - \\beta_2x_{i,2} - \\beta_3x_{i,3} -...- \\beta_p x_{i,p})^2$$\n",
    "\n",
    "and factored:\n",
    "\n",
    "$$[y_i - \\beta_0 - \\sum_{i=1}^{p}(\\beta_i x_{i,p})]2$$\n",
    "\n",
    "$$y_i[y_i - \\beta_0 - \\sum_{i=1}^{p}(\\beta_i x_{i,p})] - \\beta_0[y_i - \\beta_0 - \\sum_{i=1}^{p}(\\beta_i x_{i,p})] - \\sum_{i=1}^{p}[y_i - \\beta_0 - \\sum_{i=1}^{p}(\\beta_i x_{i,p})]$$\n",
    "\n",
    "$$y_i^2 - y_i\\beta_0 - y_i\\sum_{i=1}^{p}(\\beta_i x_{i,p}) - y_i \\beta_0 + \\beta_0^2 + \\beta_0 \\sum_{i=1}^{p}(\\beta_i x_{i,p}) - y_i \\sum_{i=1}^{p}(\\beta_i x_{i,p}) + \\beta_0 \\sum_{i=1}^{p}(\\beta_i x_{i,p}) + [\\sum_{i=1}^{p}(\\beta_i x_{i,p})]^2 $$\n",
    "\n",
    "$$ y_i^2 + \\beta_0^2 + [\\sum_{i=1}^{p}(\\beta_i x_{i,p})]^2 + 2\\beta_0 \\sum_{i=1}^{p}(\\beta_i x_{i,p}) - 2y_i\\beta_0 - 2y_i \\sum_{i=1}^{p}(\\beta_i x_{i,p})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be simplified further to reduce the headache that computing the partial derivatives will cause, first that squared summation needs to be dealt with.\n",
    "\n",
    "The sum of a square is equal to the sum of the squares of all terms, plus the sum of the double products of the terms multiplied by two (from [planetmath.org](https://planetmath.org/squareofsum)):\n",
    "\n",
    "$$(\\sum_{i=1}^{n} x_i)^2 = \\sum_{i=1}^{n} x_i^2 + 2 \\sum_{i<j}^{n} x_i x_j$$\n",
    "\n",
    "Which in the model, can be expressed as:\n",
    "\n",
    "$$(\\sum_{i=1}^{n} \\beta_i x_{i,p})^2 + \\sum_{i<j}^{n} (\\beta_i x_{i,p})(\\beta_i x_{j,p})$$\n",
    "\n",
    "or \n",
    "\n",
    "$$(\\sum_{i=1}^{n} \\beta_i x_{i,p})^2 + \\sum_{i<j}^{n} \\beta_i^2(x_{i,p})(x_{j,p})$$\n",
    "\n",
    "Is this ^^^ correct? Or should it be $\\beta_i*\\beta_j$ in the double product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This expanded expression seems sufficient for calculating the partial derivatives for $\\hat{\\beta}$. \n",
    "\n",
    "If the partial derivative with respect to $\\beta_1$ calculated, every term that is not multiplied by \\beta_1 will become 0, meaning the terms containing $\\beta_2, \\beta_3, ..., \\beta_p$ are dropped from the expression. This assumption holds for $\\beta_2$-$\\beta_p$, therefore the summations that _are not squared_ can be removed from the expression. [ADD WHY SQUARED SUMMATION IS KEPT]\n",
    "\n",
    "$$ y_i^2 + \\beta_0^2 + [\\sum_{i=1}^{p}(\\beta_i x_{i,p})]^2 + 2\\beta_0 \\sum_{i=1}^{p}(\\beta_i x_{i,p}) - 2y_i\\beta_0 - 2y_i \\sum_{i=1}^{p}(\\beta_i x_{i,p})$$\n",
    "\n",
    "becomes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
