{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Multiple Linear Regression in Python__\n",
    "\n",
    "By: Trevor Rowland ([dBCooper2](https://github.com/dBCooper2))\n",
    "\n",
    "Creating a Multiple Linear Regression Model from Scratch\n",
    "\n",
    "Expanding on the Simple Linear Regression notebook, this notebook aims to implement a Multivariate Linear Regression Model for use in Fama-French 3-Factor and 5-Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _References:_\n",
    "\n",
    "[Deriving Normal Equation for Multiple Linear Regression](<https://medium.com/@bhanu0925/deriving-normal-equation-for-multiple-linear-regression-85241965ee3b>) by [Bhanumathi Ramesh](https://medium.com/@bhanu0925)\n",
    "\n",
    "[Matrix Approach to Multiple Linear Regression](https://youtu.be/NzuK4iAfxhU?si=cxU-v8ZBgbA1s-FG) by [LearnChemE](https://www.youtube.com/@LearnChemE)\n",
    "\n",
    "[Matrix Form Multiple Linear Regression MLR](https://youtu.be/Imjfp1cxy6g?si=gWXnA9F_XisVzFA4) Tutorial by [Boer Commander](https://www.youtube.com/@BoerCommander)\n",
    "\n",
    "---------\n",
    "\n",
    "[Gradient Descent Tutorial](https://www.machinelearningworks.com/tutorials/gradient-descent)\n",
    "\n",
    "[Multiple Linear Regression from Scratch - Machine Learning Math & Python](https://youtu.be/fldD6fGmsQE?si=IwQntHRUuJCFB-iz) by [kai](https://www.youtube.com/@dylankailau6672)\n",
    "\n",
    "[Statistics 101: Multiple Linear Regression, The Very Basics ðŸ“ˆ](https://youtu.be/dQNpSa-bq4M?si=9vpoTxdyGzZEPGOx) by Brandon Foltz\n",
    "\n",
    "[Statistics 101: Multiple Linear Regression, Data Preparation](https://youtu.be/2I_AYIECCOQ?si=axl8PUqk-JUR8QQn) by Brandon Foltz\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Introduction:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models take a series of predictor(X) variables and a single response(Y) variable, and estimates a line of best fit that can be used to predict unknown response variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the Multiple Regression Model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+...+\\beta_px_p + \\epsilon_i$$\n",
    "\n",
    "For the multiple regression equation, $\\epsilon_i$ is assumed to be 0, however this noise does exist and needs to be accounted for in the analysis of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Deriving the Gradient Descent Formula_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simple linear regression model, it was easy to calculate the gradient descent as there were only 2 partial derivatives to calculate. For the multiple regression model, there are 4 and 6 partial derivatives for the Fama-French Models. The derivatives are with respect to the 3-5 predictor variables, and with respect to the alpha, or y-intercept of the regression line.\n",
    "\n",
    "Additionally, for the Fama-French Regression Class and future Regression Models, it is necessary to have an abstract Regression Model that can handle an indeterminate number of predictor variables. This means the derivation of the error function must be done in a way that can be translated to an array of size $n$ in Python. This requires the use of Matrices to simplify the calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multiple Linear Regression Model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+...+\\beta_px_p + \\epsilon_i$$\n",
    "\n",
    "Which can be translated into the Matrix Form:\n",
    "\n",
    "$$\n",
    "Y_i = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 & \\beta_1 & ... & \\beta_p\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "X_0 \\\\\n",
    "X_1 \\\\\n",
    "... \\\\\n",
    "X_p \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    ", X_0 = 1\n",
    "$$\n",
    "\n",
    "Setting $X_0 = 1$ allows the matrices to be the same size, which simplifies the calculations by including the Y-intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the Simple Linear Regression Model, the Ordinary Least Squares approach will be used to estimate the $\\beta$ coefficients for $\\beta_0 to \\beta_p$.\n",
    "\n",
    "The Least Squares Estimate $\\hat{\\beta}$ is the solution for $\\beta$ when the partial derivative of the Error Function is 0, and is calculated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Deriving the Least Squares Estimator_ $\\hat{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was done using [Bhanumathi Ramesh's](https://medium.com/@bhanu0925) article [Deriving Normal Equation for Multiple Linear Regression](<https://medium.com/@bhanu0925/deriving-normal-equation-for-multiple-linear-regression-85241965ee3b>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Model and the SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multiple Linear Regression Model: $$y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+...+\\beta_px_p + \\epsilon_i$$\n",
    "\n",
    "can be expressed in matrix form as:\n",
    "\n",
    "$$\\hat{Y} = X \\beta + \\mathcal{E}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Formula for the Sum Squared Errors(SSE) is: $$E = SSE = \\sum_{n}^{i=1}(Y_i-\\hat{Y_i})$$\n",
    "\n",
    "Another way to represent the Error Function is to break the summation into matrices:\n",
    "\n",
    "$$E =\n",
    "\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\hat{y_1}& \n",
    "y_2 - \\hat{y_2}&  \n",
    "... &  \n",
    "y_n - \\hat{y_n}\n",
    "\\end{bmatrix} \n",
    "\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\hat{y_1}\\\\\n",
    "y_2 - \\hat{y_2}\\\\\n",
    "\\vdots \\\\\n",
    "y_n - \\hat{y_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Which is equivalent to:\n",
    "\n",
    "$$E = \\hat{\\mathcal{E}}^T\\hat{\\mathcal{E}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Algebra, the transpose of a sum can be decomposed in the following ways:\n",
    "\n",
    "$$(A+B)^T = A^T+B^T$$\n",
    "\n",
    "$$(A-B)^T = A^T-B^T$$\n",
    "\n",
    "Which means the transpose operator in $E = \\hat{\\mathcal{E}}^T\\hat{\\mathcal{E}}$ can be distributed, making the function:\n",
    "\n",
    "$$ E = (Y^T-\\hat{Y}^T)(Y-\\hat{Y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting the matrix form $\\hat{Y} = X \\beta$ into the error function returns:\n",
    "\n",
    "$$ E = (Y^T-(X \\beta)^T)(Y-(X \\beta))$$\n",
    "\n",
    "$$ E = Y^T Y - Y^T X \\beta - Y(X \\beta)^T + (X \\beta)^T (X \\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to finish simplifying the equations, the following terms must be proven equal in order to simplify into the solution $\\hat{\\beta} = (X^T X^{-1})(X^T Y)$:\n",
    "\n",
    "$$(X \\beta)^T Y = Y^T (X \\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Y = A, X \\beta=B$:\n",
    "\n",
    "Therefore the equation $(X \\beta)^T Y = Y^T (X \\beta)$ becomes $A^T B = B^T A$\n",
    "\n",
    "By Linear Algebra, \n",
    "\n",
    "$$ (AB)^T = B^T A^T, (A+B)^T = A^T + B^T $$\n",
    "$$ (A^T B)^T = B^T A, (A-B)^T = A^T - B^T $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ A^T B = B^T A = (A^T B)^T $$\n",
    "\n",
    "$$Y^T (X \\beta)  = (Y^T (X \\beta))^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting this back into the SSE Equation allows it to be simplified:\n",
    "\n",
    "$$ E = Y^T Y - Y^T X \\beta - Y(X \\beta)^T + (X \\beta)^T (X \\beta)$$\n",
    "\n",
    "$$ E = Y^T Y - 2Y^T X \\beta + (X \\beta)^T (X \\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Partial Derivative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, the partial derivative with respect to $\\beta$ can be applied:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} [Y^T Y - 2Y^T X \\beta + (X \\beta)^T (X \\beta)] $$\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial \\beta} = 0 - 2Y^T X + 2X^T \\beta^T X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve for $\\hat{\\beta}$, set the derivative equal to 0 and solve:\n",
    "\n",
    "$$ 0 = 0 - 2Y^T X + X^T \\hat{\\beta}^T X$$\n",
    "\n",
    "$$ 2Y^T X = X^T \\hat{\\beta}^T X$$\n",
    "\n",
    "$$ \\hat{\\beta}^T = \\frac{2Y^T X}{2X^T X}  = (Y^T X)(X^T X)^{-1}$$\n",
    "\n",
    "$$ \\hat{\\beta} = [(Y^T X)(X^T X)^{-1}]^T $$\n",
    "\n",
    "$$ \\hat{\\beta} = (Y^T X)^T [(X^T X)^{-1}]^T $$\n",
    "\n",
    "$$ \\hat{\\beta} = (X^T Y) (X^T X^{-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lines up with the solution, and this $\\hat{\\beta}$ represents a matrix of the coefficients that can be solved with the predictor and response variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By solving for $\\hat{\\beta}$, the normal equations for the model have also been solved for. The normal equations are:\n",
    "\n",
    "$$X^TX\\hat{\\beta} = X^T Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _The Gradient Descent Function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Simple Linear Regression Model, the Formula for the Gradient Descent of the slope was:\n",
    "\n",
    "$$m_{new} = m_{current} - \\frac{\\partial E}{\\partial m}$$\n",
    "\n",
    "Because the Multiple Linear Regression Model is composed of multiple slopes, a more abstract version of this equation must be constructed that uses $\\beta$ for each of the slopes, as well as the intercept. This formula can be represented as:\n",
    "\n",
    "$$\\beta_{new} = \\beta_{current} - \\frac{\\partial E}{\\partial \\beta}$$\n",
    "\n",
    "Where i is the number of estimators in the regression equation.\n",
    "\n",
    "Lastly, a learning rate, $L$, needs to be added to the equation. This will be used in the iterative steps of the Python program that will be written. This means that the completed Gradient Descent Function is:\n",
    "\n",
    "$$\\beta_{new} = \\beta_{current} - L \\frac{\\partial E}{\\partial \\beta} $$\n",
    "\n",
    "and in matrix form:\n",
    "\n",
    "$$\\hat{\\beta}_{new} = \\hat{\\beta}_{current} - L \\frac{\\partial E}{\\partial \\hat{\\beta}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative will be computed later, \n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial \\beta} = - 2Y^T X + 2X^T \\beta^T X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After speaking with Dr. Allen:\n",
    "\n",
    "- Betas should be an array of existing betas, do not substitute."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
